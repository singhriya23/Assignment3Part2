{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "lojqomylk2cy6ain4lo5",
   "authorId": "4239853551287",
   "authorName": "RIYA98",
   "authorEmail": "singh.riya2@northeastern.edu",
   "sessionId": "15469d39-6efd-4c8c-94d7-e4004177020c",
   "lastEditTime": 1740631627842
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "e7750f70-f3a5-4186-b354-ec077996b25d",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Import necessary packages\nimport logging\nfrom snowflake.snowpark.context import get_active_session\n\n# Setup Logger\nlogger = logging.getLogger(\"travel_time_metrics_logger\")\nlogger.setLevel(logging.INFO)\nch = logging.StreamHandler()\nch.setLevel(logging.INFO)\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nch.setFormatter(formatter)\nlogger.addHandler(ch)\n\n# Get Snowpark session\nsession = get_active_session()\n\n# Define target table\ndatabase_name = \"TOM_DB\"\nschema_name = \"ANALYTICS\"\ntable_name = \"TRAVEL_TIME_METRICS\"\n\nlogger.info(\"Session initialized and variables set.\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "68a20189-be9b-4fa8-bcd8-a6a4961b0448",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Function to check if the table exists\ndef table_exists(session, database_name='', schema_name='', table_name=''):\n    query = f\"\"\"\n        SELECT EXISTS (\n            SELECT 1 \n            FROM {database_name}.INFORMATION_SCHEMA.TABLES \n            WHERE TABLE_SCHEMA = '{schema_name}' \n            AND TABLE_NAME = '{table_name}'\n        ) AS TABLE_EXISTS\n    \"\"\"\n    exists = session.sql(query).collect()[0]['TABLE_EXISTS']\n    return exists\n\n# Test the function (optional)\ntable_exists_flag = table_exists(session, database_name, schema_name, table_name)\nlogger.info(f\"Does the table {table_name} exist? {table_exists_flag}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "584ab590-82ae-4124-835d-830702e62e51",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Load the harmonized_tom table\nharmonized_tom = session.table(\"TOM_DB.HARMONIZED_TOM.harmonized_tom\")\n\n# Display first few rows to inspect data\nharmonized_tom.show(5)\nlogger.info(\"Loaded harmonized_tom table.\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d6ddbcb0-e3ef-4e0c-9c8d-40cccc94d9be",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.snowpark import functions as F\n\n# Prepare final_agg DataFrame without changing the column name for RANK\nfinal_agg = (\n    session.table(\"TOM_DB.HARMONIZED_TOM.HARMONIZED_TOM\")\n    .group_by(\n        F.col(\"RANK\"), \n        F.col(\"CITY\"), \n        F.col(\"COUNTRY\")\n    ).agg(\n        F.avg(\"AVG_TIME_PER_6_MILES\").alias(\"AVG_TRAVEL_TIME\"),\n        F.max(\"CHANGE_IN_CONGESTION\").alias(\"LATEST_CHANGE\"),\n        F.avg(\"CONGESTION_LEVEL\").alias(\"AVG_CONGESTION_LEVEL\"),\n        F.avg(\"YEARLY_DELAY_HOURS\").alias(\"AVG_TIME_LOST\")\n    ).select(\n        F.col(\"RANK\"),    # Use RANK directly\n        F.col(\"CITY\"),\n        F.col(\"COUNTRY\"),\n        F.col(\"AVG_TRAVEL_TIME\"),\n        F.call_udf(\"TOM_DB.ANALYTICS.CONGESTION_CHANGE_CONSISTENCY\", F.col(\"LATEST_CHANGE\")).alias(\"CONSISTENCY_SCORE\"),\n        F.col(\"AVG_CONGESTION_LEVEL\"),\n        F.col(\"AVG_TIME_LOST\")\n    )\n)\n\n# Check schema to verify columns\nprint(final_agg.schema.names)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "834dbbe2-c91a-476c-9f79-2ca179ace199",
   "metadata": {
    "language": "python",
    "name": "cell5",
    "collapsed": false
   },
   "outputs": [],
   "source": "# If table doesn't exist, create it\nif not table_exists(session, database_name, schema_name, table_name):\n    final_agg.write.mode(\"overwrite\").save_as_table(f\"{database_name}.{schema_name}.{table_name}\")\n    logger.info(f\"Successfully created {table_name}\")\n\n# Otherwise, update it incrementally\nelse:\n    # Use RANK consistently everywhere\n    cols_to_update = {\n        f'\"{c.upper()}\"': F.col(f\"source.{c.upper()}\") for c in final_agg.schema.names\n    }\n\n    existing_table = session.table(f\"{database_name}.{schema_name}.{table_name}\")\n\n# Perform the merge operation with backticks\n    existing_table.alias(\"target\") \\\n    .merge(\n        final_agg.alias(\"source\"),\n        (F.col(\"target.`RANK`\") == F.col(\"source.`RANK`\")) &  \n        (F.col(\"target.`CITY`\") == F.col(\"source.`CITY`\")) &  \n        (F.col(\"target.`COUNTRY`\") == F.col(\"source.`COUNTRY`\")),  \n        [\n            F.when_matched().update(cols_to_update), \n            F.when_not_matched().insert(cols_to_update)\n        ]\n    )\n\n    logger.info(f\"Successfully updated {table_name}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c27622c4-09a6-46ce-ab82-948d643f3853",
   "metadata": {
    "language": "sql",
    "name": "cell7",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM TOM_DB.ANALYTICS.TRAVEL_TIME_METRICS LIMIT 10;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d8d36086-d493-4874-bd2f-33f962433b27",
   "metadata": {
    "language": "sql",
    "name": "cell6",
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE PROCEDURE TOM_DB.ANALYTICS.UPDATE_TRAVEL_TIME_METRICS()\nRETURNS STRING\nLANGUAGE PYTHON\nRUNTIME_VERSION = '3.8'\nHANDLER = 'run'\nPACKAGES = ('snowflake-snowpark-python')\nEXECUTE AS CALLER\nAS\n$$\nimport logging\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark import functions as F\n\ndef table_exists(session, database_name, schema_name, table_name):\n    query = f\"\"\"\n        SELECT EXISTS (\n            SELECT 1 \n            FROM {database_name}.INFORMATION_SCHEMA.TABLES \n            WHERE TABLE_SCHEMA = '{schema_name}' \n            AND TABLE_NAME = '{table_name}'\n        ) AS TABLE_EXISTS\n    \"\"\"\n    result = session.sql(query).collect()\n    return result[0]['TABLE_EXISTS'] if result else False\n\ndef run(session: Session):\n    try:\n        # Setup logging\n        logger = logging.getLogger(\"TRAVEL_TIME_METRICS_LOGGER\")\n        logger.setLevel(logging.INFO)\n        \n        database_name = \"TOM_DB\"\n        schema_name = \"ANALYTICS\"\n        table_name = \"TRAVEL_TIME_METRICS\"\n\n        # Prepare final_agg DataFrame without changing the column name for RANK\n        final_agg = (\n            session.table(\"TOM_DB.HARMONIZED_TOM.HARMONIZED_TOM\")\n            .group_by(\n                F.col(\"RANK\"), \n                F.col(\"CITY\"), \n                F.col(\"COUNTRY\")\n            ).agg(\n                F.avg(\"AVG_TIME_PER_6_MILES\").alias(\"AVG_TRAVEL_TIME\"),\n                F.max(\"CHANGE_IN_CONGESTION\").alias(\"LATEST_CHANGE\"),\n                F.avg(\"CONGESTION_LEVEL\").alias(\"AVG_CONGESTION_LEVEL\"),\n                F.avg(\"YEARLY_DELAY_HOURS\").alias(\"AVG_TIME_LOST\")\n            ).select(\n                F.col(\"RANK\"),    # Use RANK directly\n                F.col(\"CITY\"),\n                F.col(\"COUNTRY\"),\n                F.col(\"AVG_TRAVEL_TIME\"),\n                F.call_udf(\"TOM_DB.ANALYTICS.CONGESTION_CHANGE_CONSISTENCY\", F.col(\"LATEST_CHANGE\")).alias(\"CONSISTENCY_SCORE\"),\n                F.col(\"AVG_CONGESTION_LEVEL\"),\n                F.col(\"AVG_TIME_LOST\")\n            )\n        )\n\n        # Check schema to verify columns\n        logger.info(\"Final aggregation schema: \" + str(final_agg.schema.names))\n\n        # If table doesn't exist, create it\n        if not table_exists(session, database_name, schema_name, table_name):\n            final_agg.write.mode(\"overwrite\").save_as_table(f\"{database_name}.{schema_name}.{table_name}\")\n            logger.info(f\"Successfully created {table_name}\")\n            return f\"Successfully created {table_name}\"\n\n        # Otherwise, update it incrementally\n        else:\n            # Use RANK consistently everywhere\n            cols_to_update = {\n                f'\"{c.upper()}\"': F.col(f\"source.{c.upper()}\") for c in final_agg.schema.names\n            }\n\n            existing_table = session.table(f\"{database_name}.{schema_name}.{table_name}\")\n\n            # Perform the merge operation with backticks\n            existing_table.alias(\"target\") \\\n            .merge(\n                final_agg.alias(\"source\"),\n                (F.col(\"target.`RANK`\") == F.col(\"source.`RANK`\")) &  \n                (F.col(\"target.`CITY`\") == F.col(\"source.`CITY`\")) &  \n                (F.col(\"target.`COUNTRY`\") == F.col(\"source.`COUNTRY`\")),  \n                [\n                    F.when_matched().update(cols_to_update), \n                    F.when_not_matched().insert(cols_to_update)\n                ]\n            )\n\n            logger.info(f\"Successfully updated {table_name}\")\n            return f\"Successfully updated {table_name}\"\n    \n    except Exception as e:\n        logger.error(f\"Error: {str(e)}\")\n        return f\"Error: {str(e)}\"\n$$;\n",
   "execution_count": null
  }
 ]
}